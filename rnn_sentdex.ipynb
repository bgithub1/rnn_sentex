{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentdex Rnn with Keras\n",
    "### This is an ipynb notebook that attempts to speed up and simplify the creation of sequence data for the RNN that Harrison created in :\n",
    "#### See (https://pythonprogramming.net/crypto-rnn-model-deep-learning-python-tensorflow-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from collections import deque\n",
    "from sklearn import preprocessing\n",
    "import datetime,time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RNN, Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization,Flatten,SimpleRNN\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Data\n",
    "* You will create a pandas DataFrame which will hold price and volume data for one minute currency bars.\n",
    "* You must transform the rows of the DataFrame into 4 numpy arrays:\n",
    " 1. train_x: shape = (num_training_rows-SEQ_LEN,SEQ_LEN,NUM_FEATURES)\n",
    " 2. train_y: shape = (num_training_rows-SEQ_LEN,SEQ_LEN,1)  _We assume that your target is a scaler number._\n",
    " 3. validation_x: shape = (num_validation_rows-SEQ_LEN,SEQ_LEN,NUM_FEATURES)\n",
    " 4. validation_y: shape = (num_validation_rows-SEQ_LEN,SEQ_LEN,1) \n",
    "___\n",
    "### IMPORTANT\n",
    "* **The currency data for the project is in the folder ./data/crypto_data .** \n",
    "* **_I HAVE ADDED HEADERS to each of the original csv files._** \n",
    "* **The original csv files without the headers are in a folder called ./data/crypto_data_original .**\n",
    "* **The DataFrame that combines all of the currency price and volume data gets assembled in (2.1).**\n",
    "___\n",
    "### ALSO IMPORTANT\n",
    "* **Run the cells in this notebook several times.  I have seen instances where, on the very first run of the notebook, the accurancy numbers are much lower than in subsequent runs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### (1.1) Define parameters:\n",
    "___\n",
    "1. SCALING_TO_USE: one of ['actual', 'pct', or 'minmax']\n",
    " * 'actual' : use the totally unscalled price and volume data for each currency pair\n",
    " * 'pct' : use percent change data for price and volume for each currency pair\n",
    " * 'minmax' : use percent change data that is further scaled to be between 0 and 1\n",
    "2. SEQ_LEN: You divide your original feature data into an array of shape (-1,SEQ_LEN,num_features)\n",
    "3. FUTURE_PERIOD_PREDICT: the number of rows beyond the last row of each sequence that you use to create your target variable.  If your first sequence of training data comes from rows\n",
    "4. RATIO_TO_PREDICT: the currency that you will use as the target.  You will try to predict if this currency price goes up or down afer the number of periods in FUTURE_PERIOD_PREDICT.\n",
    "5. EPOCHS: Number of epochs to input into the keras model\n",
    "6. BATCH_SIZE: number of batches of shape (SEQ_LEN,NUM_FEATURES) for x data and (SEQ_LEN,1) for y data (labels).\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these values\n",
    "# Chose as value for SCALING_TO_USE among ['actual', 'pct', or 'minmax']\n",
    "SCALING_TO_USE = 'minmax' \n",
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"LTCUSD\" # change this to use different currencies as the target\n",
    "EPOCHS = 3  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "USE_LSTM = False # If true, use keras.layers.LSTM as first layer.  Otherwise, use keras.layers.SimpleRNN\n",
    "\n",
    "# DO NOT CHANGE THE VALUES BELOW\n",
    "DATA_FOLDER = './data/crypto_data'\n",
    "CURRENCY_FILE_NAMES = os.listdir(DATA_FOLDER) \n",
    "CURRENCY_LIST = [c.replace('-','').replace('.csv','')for c in CURRENCY_FILE_NAMES]\n",
    "if RATIO_TO_PREDICT not in CURRENCY_LIST:\n",
    "    raise ValueError(f'the currency {RATIO_TO_PREDICT} is not in the valid currency list {CURRENCY_LIST}')\n",
    "print(f'possible currencies to use below are: {[c.replace(\"-\",\"\").replace(\".csv\",\"\") for c in CURRENCY_FILE_NAMES]}')\n",
    "\n",
    "\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "print(f'using {SCALING_TO_USE} as scaling')\n",
    "print(f'using {RATIO_TO_PREDICT} currency as a target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.1) Create DataFrame of close and volume for each currency\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_future = FUTURE_PERIOD_PREDICT\n",
    "# define variable to hold final DataFrame with combined currency prices and volumes, and target values\n",
    "df_curr = None\n",
    "# loop through each currency csv file\n",
    "for i in range(len(CURRENCY_LIST)):\n",
    "    cfn = CURRENCY_FILE_NAMES[i]\n",
    "    c  = CURRENCY_LIST[i]\n",
    "    df_temp = pd.read_csv(f'{DATA_FOLDER}/{cfn}')\n",
    "    df_temp = df_temp.rename(columns={'close':f'close_{c}','volume':f'volume_{c}'})\n",
    "    df_temp.index = df_temp.timestamp\n",
    "    df_temp = df_temp[[f'close_{c}',f'volume_{c}']]\n",
    "    if i ==0:\n",
    "        df_curr = df_temp.copy()\n",
    "    else:\n",
    "        df_curr = pd.concat([df_curr,df_temp],axis=1)\n",
    "close_string = f'close_{RATIO_TO_PREDICT}'\n",
    "future_close_string = f'close_{RATIO_TO_PREDICT}_future'\n",
    "df_curr[future_close_string] = df_curr[close_string].shift(-days_in_future)\n",
    "df_curr['target'] = df_curr[future_close_string] >= df_curr[close_string]\n",
    "df_curr['hour'] = [datetime.datetime.fromtimestamp(d).hour for d  in df_curr.index]\n",
    "df_curr['minute'] = [datetime.datetime.fromtimestamp(d).minute for d  in df_curr.index]\n",
    "df_curr['year'] = [datetime.datetime.fromtimestamp(d).year for d  in df_curr.index]\n",
    "df_curr['month'] = [datetime.datetime.fromtimestamp(d).month for d  in df_curr.index]\n",
    "df_curr['day'] = [datetime.datetime.fromtimestamp(d).day for d  in df_curr.index]\n",
    "\n",
    "# get all columns except future_close_string\n",
    "cols = list(filter(lambda co: co!=future_close_string,df_curr.columns.values))\n",
    "\n",
    "\n",
    "df_curr = df_curr[(df_curr.hour>=8) & (df_curr.hour<=17)]\n",
    "df_curr['yyyymmdd'] = df_curr.year*100*100 + df_curr.month*100 + df_curr.day\n",
    "df_curr = df_curr[['yyyymmdd','hour','minute']+cols]\n",
    "\n",
    "len(df_curr),df_curr.columns.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### (3.1) Create separate training and validation DataFrames\n",
    "Create 2 DataFrames:\n",
    "1. df_train\n",
    "2. df_test\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all yyyymmdd's  in df_curr.yyyymmdd\n",
    "all_days = np.array(list(set(df_curr.yyyymmdd.as_matrix().reshape(-1))))\n",
    "# establish a training size percent\n",
    "train_perc = .9\n",
    "# get the index into the array all_days, which represents the last training day\n",
    "last_train_index = int(len(all_days)*train_perc)\n",
    "# get training days\n",
    "train_days = all_days[:last_train_index]\n",
    "# get df_train\n",
    "df_train = df_curr[df_curr.yyyymmdd.isin(train_days)]\n",
    "# get test days\n",
    "test_days = all_days[last_train_index:]\n",
    "# get df_test\n",
    "df_test = df_curr[df_curr.yyyymmdd.isin(test_days)]\n",
    "# print various lengths to make sure the df_train and df_test DataFrames sum to df_curr\n",
    "print(f'df_curr:{len(df_curr)}, df_train:{len(df_train)}, df_test:{len(df_test)}, should be 0: {len(df_curr)-(len(df_train)+len(df_test))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### (3.2) normalize data\n",
    "For both training and validation data, provide return a dictionary with 3 DataFrames:\n",
    "1. A DataFrame of actual prices, and the actual target values\n",
    "2. A DataFrame with percentage prices, and the actual target values\n",
    "3. A DataFrame with min/max normalized percentage of the prices, and the actual (not normalized) target values.  \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    num_cols = list(filter(lambda c:'close' in c or 'volume' in c,df_train.columns.values))\n",
    "    dft = df.copy() #[['yyyymmdd','hour','minute']+num_cols]\n",
    "    dft.target = dft.target.astype(int)\n",
    "    dft = dft[~dft.isnull().any(axis=1)]\n",
    "    dft_actual = dft.copy()\n",
    "    dft[num_cols] = dft[num_cols].pct_change()\n",
    "    dft = dft[~dft.isnull().any(axis=1)]\n",
    "    dft_pct = dft.copy()\n",
    "    dft[num_cols] = (dft[num_cols] - dft[num_cols].min()) / (dft[num_cols].max() - dft[num_cols].min())\n",
    "    dft_minmax = dft.copy()\n",
    "    return {'actual':dft_actual,'pct':dft_pct,'minmax':dft_minmax}\n",
    "\n",
    "training = normalize_df(df_train)\n",
    "testing = normalize_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3.3) Show the head of the actual, pct and minmax training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['actual'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['pct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['minmax'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "###  (3.4) Create sequences from the minmax DataFrames\n",
    "From the input DataFrame df, a new DataFrame with::\n",
    "1. an 'x' column where each cell is an array of \"time sequences\" (shape of array in cell = (SEQ_LEN,num_x_cols))\n",
    "2. a 'y' column where each cell is an array of \"time sequences\" (shape of array in cell = (SEQ_LEN,1))\n",
    "___\n",
    "_**These methods assume that the input DataFrame has a millisecond timestamp in he index of each DataFrame row.**_\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(df,return_full_y_sequences):\n",
    "    '''\n",
    "    '''\n",
    "    # get the price and volume column names\n",
    "    num_cols = list(filter(lambda c: 'close' in c or 'volume' in c,df.columns.values))\n",
    "    # copy the input DataFrame\n",
    "    df2 = df.copy()\n",
    "    # Get the timestamps from the DataFrame's index\n",
    "    df2['ts'] = df2.index\n",
    "    df2.index = list(range(len(df2)))\n",
    "    dfy = df2[['target']]\n",
    "    dfx = df2.drop(['target'],axis=1)\n",
    "    dx = dfx.as_matrix()\n",
    "    \n",
    "    dy = dfy.as_matrix()\n",
    "    s = len(dx)\n",
    "    sl = 60\n",
    "    last_col = len(df2.columns.values) -1\n",
    "    x_sequences = [dx[i:None if i+sl>=s else i+sl] for i in range(len(dx)-(sl-1))]\n",
    "    y_sequences = [dy[i:None if i+sl>=s else i+sl] for i in range(len(dy)-(sl-1))]\n",
    "    x_seq_dicts = [{'x':x_sequences[i]} for i in list(range(len(x_sequences)))]\n",
    "    if return_full_y_sequences:\n",
    "        y_seq_dicts = [{'y':y_sequences[i]} for i in list(range(len(y_sequences)))]\n",
    "    else:\n",
    "        y_seq_dicts = [{'y':y_sequences[i][sl-1]} for i in list(range(len(y_sequences)))]\n",
    "    dfx = pd.DataFrame(x_seq_dicts)\n",
    "    dfy = pd.DataFrame(y_seq_dicts)\n",
    "    df_seq = dfx.join(dfy)\n",
    "    return df_seq\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "def create_daily_sequences(df,return_full_y_sequences):\n",
    "    '''\n",
    "    Given and input DataFrame df, run create_sequences on subsets of df \n",
    "       where each subset contains data for only a specific day.\n",
    "    Combine all of the specific \"daily\" calls to create_sequences into a single x and y array.\n",
    "    '''\n",
    "    cols_to_return = list(filter(lambda c: any([d in c for d in ['close','volume','target']]),df.columns.values))\n",
    "    df_seqs = None\n",
    "    all_days = np.array(list(set(df.yyyymmdd)))\n",
    "    for yyyymmdd in tqdm(all_days):\n",
    "        df_this_day = df[df.yyyymmdd==yyyymmdd][cols_to_return]\n",
    "        df_this_seqs = create_sequences(df_this_day,return_full_y_sequences)\n",
    "        if df_seqs is None:\n",
    "            df_seqs = df_this_seqs.copy()\n",
    "        else:\n",
    "            if len(df_this_seqs)>0:\n",
    "                df_seqs = df_seqs.append(df_this_seqs)\n",
    "    return df_seqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.5) Run create_daily_sequences to get final input data for the RNN\n",
    "Produce 4 arrays:\n",
    "1. train_x\n",
    "2. train_y\n",
    "3. validation_x\n",
    "4. validation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_full_seqs =  USE_LSTM\n",
    "df_train_seqs = create_daily_sequences(training[SCALING_TO_USE],use_full_seqs)\n",
    "train_x = np.array([df_train_seqs['x'].values[i] for i in range(len(df_train_seqs))])\n",
    "train_y = np.array([df_train_seqs['y'].values[i] for i in range(len(df_train_seqs))])\n",
    "\n",
    "df_test_seqs = create_daily_sequences(testing[SCALING_TO_USE],use_full_seqs)\n",
    "validation_x = np.array([df_test_seqs['x'].values[i] for i in range(len(df_test_seqs))])\n",
    "validation_y = np.array([df_test_seqs['y'].values[i] for i in range(len(df_test_seqs))])\n",
    "\n",
    "txl = train_x.shape[0]//BATCH_SIZE * BATCH_SIZE\n",
    "train_x_even = train_x[:txl]\n",
    "train_y_even = train_y[:txl]\n",
    "\n",
    "vxl = validation_x.shape[0]//BATCH_SIZE * BATCH_SIZE\n",
    "validation_x_even =  validation_x[:vxl]\n",
    "validation_y_even =  validation_y[:vxl]\n",
    "\n",
    "train_x_even.shape,train_y_even.shape,validation_x_even.shape,validation_y_even.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# TO DO, implement this\n",
    "### Balance data\n",
    "Create sequences whose corrosponding target values have an equal number of 0's and 1's\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(tseqs):\n",
    "    # get the target (y) values from the tsegs['y'] array, and put it into a DataFrame for indexing\n",
    "    dfy  = pd.DataFrame({'y':tseqs['y'].reshape(-1)})\n",
    "    # split dfy into 2 DataFrames: dfy0 contains 0's, and dfy1 contains 1's\n",
    "    dfy0 = dfy[dfy.y==0]\n",
    "    dfy1 = dfy[dfy.y==1]\n",
    "    # find the smaller DataFrame\n",
    "    dfy_len = min(len(dfy0),len(dfy1))\n",
    "    # create a new DataFrame which contains an equal number of targets with 1's and 0's\n",
    "    dfy01 = dfy0.iloc[:dfy_len].append(dfy1.iloc[:dfy_len])\n",
    "    # get the index values of this new DataFrame\n",
    "    dfy_indices= dfy01.index\n",
    "    # use those index values to locate the x sequences that corrospond to the targets in dfy01\n",
    "    tsy = tseqs['y'][dfy_indices]\n",
    "    tsx = tseqs['x'][dfy_indices]\n",
    "    # return 2 arrays: 1 for the x sequences and another for the corrosponding targets\n",
    "    return tsx,tsy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run balance_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## (4.1) Now create the Keras model and run it\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "if USE_LSTM:\n",
    "    model.add(LSTM(128, return_sequences=True,stateful=True,\n",
    "              batch_input_shape=(BATCH_SIZE,train_x.shape[1],train_x.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "else:\n",
    "    model.add(SimpleRNN(128))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.00001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x_even, train_y_even,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=True,\n",
    "    validation_data=(validation_x_even, validation_y_even))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
